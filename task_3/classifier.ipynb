{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import pandas as pd\n",
    "from sklearn import svm, model_selection, metrics, linear_model, tree\n",
    "from statsmodels.tsa.statespace.varmax import VARMAX\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import RobustScaler, FunctionTransformer\n",
    "from sklearn.svm import SVC, SVR, LinearSVR\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.feature_selection import SelectPercentile, SelectKBest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "import sklearn.feature_selection as feature_selection\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Importing libraries for building the neural network\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import SGD\n",
    "from keras import regularizers      #for l2 regularization\n",
    "from keras.wrappers.scikit_learn import KerasRegressor \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import RobustScaler, FunctionTransformer\n",
    "from sklearn.svm import SVC, SVR, LinearSVR\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.feature_selection import SelectPercentile, SelectKBest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "import sklearn.feature_selection as feature_selection\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "acids = [\"R\", \"H\", \"K\", \"D\", \"E\", \"S\", \"T\", \"N\", \"Q\", \"C\", \"U\", \"G\",\"P\", \"A\", \"I\", \"L\", \"M\", \"F\",\"W\", \"Y\", \"V\"]\n",
    "def seq_to_vec(seq):\n",
    "    \"\"\"amino acid sequence to a number (index in the array of acids)\n",
    "       H - 1 index -> 1\n",
    "       R - 0 index -> 0\n",
    "    \"\"\"\n",
    "    vec = []\n",
    "    for word in seq:\n",
    "        vec.append(acids.index(word)) \n",
    "    return np.array(vec)\n",
    "\n",
    "def hot_encoder(elem):\n",
    "    \"\"\"amino acid sequence to a hot vector of dim 21 (one in the place of index-number in the array of acids)\n",
    "       H - 1 index -> 010000000000000000000\n",
    "    \"\"\"\n",
    "    hot_vector = np.zeros(21)\n",
    "    index = acids.index(elem)\n",
    "    hot_vector[index] = 1\n",
    "    return hot_vector\n",
    "\n",
    "def hot_encoder_group(elem):\n",
    "    \"\"\"amino acid sequence to a hot vector of dim 21 (number of group in the place of index-number in the array of acids)\n",
    "       D - 3 index and belongs to 2-d group; i -> 000200000000000000000;\n",
    "    \"\"\"\n",
    "    hot_vector = np.zeros(21)\n",
    "    index = acids.index(elem)\n",
    "    group_index = group_amino(index + 1)\n",
    "    hot_vector[index] = group_index + 1\n",
    "    return hot_vector\n",
    "\n",
    "def seq_to_hot(seq):\n",
    "    vec = []\n",
    "    for word in seq:\n",
    "        vec.append(hot_encoder(word)) \n",
    "    return np.array(vec)\n",
    "\n",
    "def seq_to_hot_tensor(seq):\n",
    "    vec = []\n",
    "    for word in seq:\n",
    "        #!!!\n",
    "        vec.append(hot_encoder(word)) \n",
    "    return np.array(vec).reshape(4, 21)\n",
    "\n",
    "def hot_group_amino(letter):\n",
    "    hot_vector = np.zeros(4)\n",
    "    index = acids.index(letter)\n",
    "    group_index = group_amino(index + 1) - 1\n",
    "    hot_vector[group_index] = 1\n",
    "    return hot_vector\n",
    "\n",
    "def seq_to_hot_group(seq):\n",
    "    vec = []\n",
    "    for word in seq:\n",
    "        vec.append(hot_encoder_group(word)) \n",
    "    return np.array(vec)\n",
    "\n",
    "def global_hot(seq):\n",
    "    global_hot = []\n",
    "    for elem in seq:\n",
    "#         print(\"elem\",elem)\n",
    "        global_hot += hot_encoder(elem).tolist()\n",
    "    for elem in seq:\n",
    "        global_hot += hot_group_amino(elem).tolist()\n",
    "    return np.array(global_hot)\n",
    "#     result = np.stack(global_hot, axis = 0)\n",
    "#     return result\n",
    "\n",
    "def group_amino(idx):\n",
    "    \"\"\"return chemical groupping for as side chain groups\n",
    "       check this: https://commons.wikimedia.org/wiki/File:Amino_Acids-wide.svg\n",
    "    \"\"\"\n",
    "    if (idx == 0 or idx == 1 or idx == 2 or idx == 3 or idx == 4):\n",
    "        return 1\n",
    "    elif (idx == 5 or idx == 6 or idx == 7 or idx == 8):\n",
    "        return 2\n",
    "    elif (idx == 9 or idx == 10 or idx == 11 or idx == 12):\n",
    "        return 3\n",
    "    elif (idx == 13 or idx == 14 or idx == 15 or idx == 16 or idx == 17 or idx == 18 or idx == 19 or idx == 20):\n",
    "        return 4\n",
    "    \n",
    "def acids_to_channels(seq):\n",
    "    \"\"\"creates 4*4 feature tensor\n",
    "       every group corresponds to a special channel. Firstly we encode protein into a seqience of indices\n",
    "       of amino acids array.\n",
    "       Eg: 2348\n",
    "       In every \"subdimension - channel\" we leave just idxs of elements belonging to this group\n",
    "       Eg 2 and 3 belongs to the first group, 4 to 2d and 8 to 4-th:\n",
    "       2300\n",
    "       0040\n",
    "       0000\n",
    "       0008\n",
    "    \"\"\"\n",
    "    amino_seq = seq_to_vec(seq)\n",
    "    tensor = np.zeros((4, 4))\n",
    "    for group in range(4):\n",
    "        for elem in range(4):\n",
    "            if(group_amino(amino_seq[elem]) == group):\n",
    "                tensor[group, elem] = amino_seq[elem]\n",
    "    return tensor\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "\n",
    "def transform_data(X):\n",
    "    '''transforms data matrix and creates 8 features:\n",
    "       4 with amino acid name number and 4 with side chain group\n",
    "    '''\n",
    "    X['first_acid'] = X['Sequence'].transform(lambda x: acids.index(x[0]))\n",
    "    X['second_acid'] = X['Sequence'].transform(lambda x: acids.index(x[1]))\n",
    "    X['third_acid'] = X['Sequence'].transform(lambda x: acids.index(x[2]))\n",
    "    X['fourth_acid'] = X['Sequence'].transform(lambda x: acids.index(x[3]))\n",
    "\n",
    "    X['first_group'] = X['Sequence'].transform(lambda x: group_amino(acids.index(x[0])))\n",
    "    X['second_group'] = X['Sequence'].transform(lambda x: group_amino(acids.index(x[1])))\n",
    "    X['third_group'] = X['Sequence'].transform(lambda x: group_amino(acids.index(x[2])))\n",
    "    X['fourth_group'] = X['Sequence'].transform(lambda x: group_amino(acids.index(x[3])))\n",
    "    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"data/train.csv\")\n",
    "X = transform_data(X)\n",
    "X_data = X.drop(columns = ['Sequence', 'Active'])\n",
    "y_all = X['Active'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals  = ['first_acid', 'second_acid', 'third_acid', 'fourth_acid', 'first_group', 'second_group',\n",
    "                'third_group', 'fourth_group']\n",
    "\n",
    "x_train = X_data\n",
    "y_train = y_all\n",
    "score_summary = []\n",
    "indexes_of_categories = [x_train.columns.get_loc(col) for col in categoricals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = LGBMClassifier(categorical_feature=indexes_of_categories)\n",
    "\n",
    "gridParams = {\n",
    "    'n_estimators': [40, 100, 200, 500, 1000],\n",
    "    'reg_alpha' : [0.001, 0.1, 1, 10, 100],\n",
    "    'reg_lambda' : [0.1, 1, 10, 30],\n",
    "    'learning_rate': [0.0001, 0.005],\n",
    "    'num_leaves': [16, 31, 63],\n",
    "    }\n",
    "\n",
    "grid = GridSearchCV(mdl, gridParams,\n",
    "                    verbose=0,\n",
    "                    cv=5,\n",
    "                    n_jobs=2)\n",
    "\n",
    "#select best features for lightgbm using a grid search\n",
    "grid.fit(x_train, y_train)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline of our model. We select subspace of features (here all features) and LGBMClassifier classifier\n",
    "selector = SelectPercentile(percentile=100)\n",
    "lbm = LGBMClassifier(reg_alpha=0.1, reg_ambda=1, n_estimators=1000, categorical_feature=indexes_of_categories)\n",
    "pipeline = make_pipeline(selector, lbm)\n",
    "#0.1, 50 - 93 place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test scores:\t [0.90722892 0.72572816 0.89105295 0.75933864 0.73690548] \n",
      "Train scores:\t [1.         0.84240862 1.         0.83501887 0.84065851]\n",
      "Test: 0.8041 (+/- 0.1571)\n",
      "Train: 0.9036 (+/- 0.1575)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniil/opt/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:842: UserWarning: categorical_feature keyword has been found in `params` and will be ignored.\n",
      "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
      "  .format(key))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('selectpercentile',\n",
       "                 SelectPercentile(percentile=90,\n",
       "                                  score_func=<function f_classif at 0x11c519d40>)),\n",
       "                ('lgbmclassifier',\n",
       "                 LGBMClassifier(boosting_type='gbdt',\n",
       "                                categorical_feature=[0, 1, 2, 3, 4, 5, 6, 7],\n",
       "                                class_weight=None, colsample_bytree=1.0,\n",
       "                                importance_type='split', learning_rate=0.1,\n",
       "                                max_depth=-1, min_child_samples=20,\n",
       "                                min_child_weight=0.001, min_split_gain=0.0,\n",
       "                                n_estimators=1000, n_jobs=-1, num_leaves=31,\n",
       "                                objective=None, random_state=None,\n",
       "                                reg_alpha=0.1, reg_ambda=1, reg_lambda=0.0,\n",
       "                                silent=True, subsample=1.0,\n",
       "                                subsample_for_bin=200000, subsample_freq=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cross validation with 5 folds and f1 metric score\n",
    "scores = cross_validate(pipeline, x_train, y_train,\n",
    "                            scoring='f1',\n",
    "                            return_train_score=True,\n",
    "                            cv=5,  # Already include stratified folds\n",
    "                            return_estimator=True,\n",
    "                            n_jobs=-1)  # Add parallelism to speed up validation\n",
    "print(\"Test scores:\\t\", scores['test_score'],\n",
    "        '\\nTrain scores:\\t', scores['train_score'])\n",
    "print(\"Test: %0.4f (+/- %0.4f)\" % (scores['test_score'].mean(), scores['test_score'].std() * 2))\n",
    "score_summary.append(scores['test_score'].mean())\n",
    "print(\"Train: %0.4f (+/- %0.4f)\" % (scores['train_score'].mean(), scores['train_score'].std() * 2))\n",
    "\n",
    "pipeline.fit(x_train, y_train)\n",
    "# y_pred = pipeline.predict_proba(x_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading & transforming test dataset and getting a prediction\n",
    "\n",
    "x_final = pd.read_csv('data/test.csv')\n",
    "x_final = transform_data(x_final)\n",
    "x_final = x_final.drop(columns = ['Sequence'])\n",
    "# x_final['Sequence'] = x_final['Sequence'].transform(lambda x: seq_to_hot(x))\n",
    "# x_final = np.stack(x_final['Sequence'].to_numpy())\n",
    "# x_final = np.reshape(x_final, (x_final.shape[0], -1))\n",
    "answer = pipeline.predict(x_final)\n",
    "np.savetxt(\"answer_lgbm_grid_search.csv\", answer, delimiter=\",\", fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
