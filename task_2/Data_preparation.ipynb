{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "# from mice import Mice\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "#new packages\n",
    "# from fancyimpute import KNN, NuclearNormMinimization, SoftImpute, BiScaler\n",
    "# from sklearn.impute import KNNImputer\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
    "from matplotlib.pyplot import figure\n",
    "from numpy import savetxt\n",
    "from numpy import loadtxt\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn import svm\n",
    "import mlxtend\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "import my_utils\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "import logging\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import lightgbm\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################LAbels of Tests###############################\n",
    "TESTS = ['LABEL_BaseExcess', 'LABEL_Fibrinogen', 'LABEL_AST', 'LABEL_Alkalinephos', 'LABEL_Bilirubin_total',\n",
    "         'LABEL_Lactate', 'LABEL_TroponinI', 'LABEL_SaO2',\n",
    "         'LABEL_Bilirubin_direct', 'LABEL_EtCO2']\n",
    "\n",
    "VITALS = ['LABEL_RRate', 'LABEL_ABPm', 'LABEL_SpO2', 'LABEL_Heartrate']\n",
    "\n",
    "path_train_features = \"data/train_features.csv\"\n",
    "path_train_labels = \"data/train_labels.csv\"\n",
    "path_test_features = \"data/test_features.csv\"\n",
    "x_test_raw = pd.read_csv(path_test_features)\n",
    "\n",
    "\n",
    "columns_labels_first = [\"BaseExcess\", \"Fibrinogen\", \"AST\", \"Alkalinephos\", \"Bilirubin_total\", \"Lactate\", \"TroponinI\", \"SaO2\", \"Bilirubin_direct\", \"EtCO2\"]\n",
    "columns_labels_second = [\"Sepsis\"]\n",
    "columns_labels_third = [\"RRate\", \"ABPm\", \"SpO2\", \"Heartrate\"]\n",
    "\n",
    "columns_params_test = [col for col in x_train_raw.columns if col not in  columns_labels_first + columns_labels_third + ['Time', 'pid', 'Age']]\n",
    "\n",
    "interest_col = columns_labels_first + columns_labels_second + columns_labels_third + ['Time', 'pid', 'Age']\n",
    "\n",
    "x_train_raw = pd.read_csv(path_train_features)\n",
    "\n",
    "y_train_labels = pd.read_csv(path_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = np.array([3,4,5])\n",
    "np.min(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have created features of \n",
    "\n",
    "1) Number of not none (number of tests) for the test labels and parameters (Bilirubin etc)\n",
    "\n",
    "2) The time of the last test for the 1-st subtask\n",
    "\n",
    "3) Mean of values for all values\n",
    "\n",
    "4) Min and Max for values in 3-d subtask\n",
    "\n",
    "5) Categorical variables: age and heart\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_to_imputed(pid, data_raw, label_raw):\n",
    "    #creates imputed values to a new pad.dataframe for all pid\n",
    "    data_patient = data_raw.loc[data_raw['pid'] == pid, label_raw].to_numpy()\n",
    "    data_patient = data_patient.reshape(-1, 1)\n",
    "    imp = IterativeImputer(missing_values=np.nan)\n",
    "    imputed_data_patient = imp.fit_transform(data_patient)\n",
    "    if(imputed_data_patient.shape[1] == 0):\n",
    "        data_raw.loc[data_raw['pid'] == pid, label_raw] = np.zeros(12)\n",
    "    else:\n",
    "        data_raw.loc[data_raw['pid'] == pid, label_raw] = imputed_data_patient\n",
    "            \n",
    "def imputation_pid(data_raw, pid, label_raw):\n",
    "    '''imputes data for patient with pid during all time using sklearn Iterative Imputer(12 days)\n",
    "    '''\n",
    "    data_patient = data_raw.loc[data_raw['pid'] == pid, label_raw].to_numpy()\n",
    "    data_patient = data_patient.reshape(-1, 1)\n",
    "    imp = IterativeImputer(missing_values=np.nan)\n",
    "    imputed_data_patient = imp.fit_transform(data_patient)\n",
    "    if(imputed_data_patient.shape[1] == 0):\n",
    "        return np.zeros(12)\n",
    "    else:\n",
    "        return imputed_data_patient\n",
    "            \n",
    "def _get_value(data, pid: int, Label: str):\n",
    "    \"\"\"get value in the dataframe data at pid and Label\n",
    "    \"\"\"\n",
    "    return data.loc[data['pid'] == pid, Label].to_numpy()\n",
    "\n",
    "def mean_generator(pid, old_data, new_data, old_name, new_name):\n",
    "    '''calculates mean of features (old_name) from impured raw data (old_data)\n",
    "       and puts them to a new engeneered feature set (new_name)\n",
    "    '''\n",
    "    new_values = np.mean(imputation_pid(old_data, pid, old_name))\n",
    "    new_data.loc[new_data['pid'] == pid, new_name] = new_values\n",
    "\n",
    "def max_generator(pid, old_data, new_data, old_name, new_name):\n",
    "    '''calculates max of features (old_name) from impured raw data (old_data)\n",
    "       and puts them to a new engeneered feature set (new_name)\n",
    "    '''\n",
    "    new_values = np.max(imputation_pid(old_data, pid, old_name))\n",
    "    new_data.loc[new_data['pid'] == pid, new_name] = new_values\n",
    "    \n",
    "def min_generator(pid, old_data, new_data, old_name, new_name):\n",
    "    '''calculates min of features (old_name) from impured raw data (old_data)\n",
    "       and puts them to a new engeneered feature set (new_name)\n",
    "    '''\n",
    "    new_values = np.min(imputation_pid(old_data, pid, old_name))\n",
    "    new_data.loc[new_data['pid'] == pid, new_name] = new_values\n",
    "    \n",
    "\n",
    "def num_not_none_generator(pid, old_data, new_data, old_name, new_name):\n",
    "    '''calculates number of NONE in feature (old_name) from impured raw data (old_data)\n",
    "       and puts them to a new engeneered feature set (new_name) - (new_data)\n",
    "    '''\n",
    "    #for columns from 1-st subtask\n",
    "    new_values = 12 - np.sum(np.isnan(old_data.loc[old_data['pid'] == pid, old_name].to_numpy())) \n",
    "    new_data.loc[new_data['pid'] == pid, old_name] = new_values\n",
    "    \n",
    "def num_last_not_none_generator(pid, old_data, new_data, old_name, new_name):\n",
    "    '''calculates the time(day) of last not NONE in feature (old_name) from impured raw data (old_data)\n",
    "       and puts them to a new engeneered feature set (new_name) - (new_data)\n",
    "    '''\n",
    "    #for columns from 1-st subtask \n",
    "    #put the day of last test\n",
    "   \n",
    "    none_id = np.isnan(old_data.loc[old_data['pid'] == pid, old_name].to_numpy())\n",
    "    result = np.where(none_id == False)\n",
    "    if (result[0].shape[0] == 0):\n",
    "        new_data.loc[new_data['pid'] == pid, new_name] = 0\n",
    "    else:\n",
    "        new_data.loc[new_data['pid'] == pid, new_name] = old_data.loc[old_data['pid'] == pid, 'Time'].to_numpy()[result[0][-1]]\n",
    "\n",
    "            \n",
    "def mean_gen_simple(data, new_data, name, new_name):\n",
    "    '''attempt to use groupby feature\n",
    "    '''\n",
    "    new_data[new_name] = data.groupby(['pid'])[name].transform(lambda x: np.mean(x))\n",
    "\n",
    "def max_gen_simple(data, new_data, name, new_name):\n",
    "    '''max of data\n",
    "    '''\n",
    "    new_data[new_name] = data.groupby(['pid'])[name].transform(lambda x: np.max(x))\n",
    "    \n",
    "def min_gen_simple(data, new_data, name, new_name):\n",
    "    '''min of data\n",
    "    '''\n",
    "    new_data[new_name] = data.groupby(['pid'])[name].transform(lambda x: np.min(x))\n",
    "    \n",
    "def feature_engineer_age(train):\n",
    "    '''generates \"categorical variables of Age\"\n",
    "    '''\n",
    "    train.loc[train['Age'] >=65, 'custom_age'] = 2\n",
    "    train.loc[train['Age'] <20, 'custom_age'] = 0\n",
    "    train.loc[(train['Age'] >=20) & (train['Age'] <65), \n",
    "            'custom_age'] = 1\n",
    "    return train\n",
    "\n",
    "\n",
    "def feature_engineer_hr(train, label_old):\n",
    "    #label_old - Mean_Heartrate\n",
    "    '''generates \"categorical variables of Heart Rate\"\n",
    "    '''\n",
    "    train.loc[train[label_old] >= 100,\n",
    "            'custom_hr'] = 1\n",
    "    train.loc[train[label_old] < 100,\n",
    "            'custom_hr'] = 0\n",
    "    return train\n",
    "\n",
    "\n",
    "#for columns from first task num_not_none_generator and num_last_not_none_generator\n",
    "#for columns from the third task mean_generator\n",
    "\n",
    "\n",
    "#generate pd dataframe with pid and features (mean for third task, number not none,last not tone for first task)\n",
    "#use this x for sepsis in 2 task\n",
    "#     values_patient = data.loc[data['pid'] == pid, Label].to_numpy()\n",
    "\n",
    "def dataset_preparation(data, name_eng_features):\n",
    "    \n",
    "    \n",
    "    #creation of an empty matrix\n",
    "    pids = np.unique(data['pid'].to_numpy())\n",
    "    data_eng = pd.DataFrame(columns = ['pid'])\n",
    "    data_eng['pid'] = pids\n",
    "    #add age column\n",
    "    age_data = [_get_value(data, pid, \"Age\")[0] for pid in pids]\n",
    "    data_eng['Age'] = age_data\n",
    "    \n",
    "    #defining columns for future feature calculation\n",
    "    not_none_columns = columns_labels_first + columns_params_test\n",
    "    mean_features = columns_labels_first + columns_labels_third + columns_params_test\n",
    "    max_min_features = columns_labels_third\n",
    "    \n",
    "    for pid in pids:\n",
    "\n",
    "    #features of number not none & time of the last not none\n",
    "    \n",
    "#     not_none_columns = columns_labels_first \n",
    "        for label in not_none_columns:\n",
    "            new_label_1 = \"Not_none_\" +  label\n",
    "            new_label_2 = \"Last_n_none_\" + label\n",
    "            num_last_not_none_generator(pid, data, data_eng, label, new_label_1)\n",
    "            num_not_none_generator(pid, data, data_eng, label, new_label_2)\n",
    "\n",
    "\n",
    "        #calculation of mean\n",
    "        for label in mean_features:\n",
    "            new_label = \"Mean_\" + label\n",
    "            mean_generator(pid, data, data_eng, label, new_label)\n",
    "\n",
    "\n",
    "        for label in max_min_features:\n",
    "            new_label = \"Min_\" + label\n",
    "            max_generator(pid, data, data_eng, label, new_label)\n",
    "            new_label = \"Max_\" + label\n",
    "            min_generator(pid, data, data_eng, label, new_label)\n",
    "\n",
    "        \n",
    "        feature_engineer_age(data_eng)\n",
    "        feature_engineer_hr(data_eng, 'Mean_Heartrate')\n",
    "\n",
    "    \n",
    "    data_eng.to_csv(name_eng_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write features to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_preparation(x_train_raw,  'train_all_eng_features.csv')\n",
    "dataset_preparation(x_test_raw,  'test_all_eng_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check what we have generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_check = pd.read_csv('train_all_eng_features.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18995, 106)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_check.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
